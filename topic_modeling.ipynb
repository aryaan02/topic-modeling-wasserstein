{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplementation of _Topic Modeling with Wasserstein Autoencoders_ by Feng Nan et al. (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Members: Aryaan Khan, Yuhang Cui, and Raymond Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.distributions import Dirichlet\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "categories = None\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Accessing the text data\n",
    "texts = data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Re-create document from tokens\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to each document in the dataset\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Example of a processed text\n",
    "print(processed_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Review the shape of the TF-IDF matrix\n",
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_topics):\n",
    "        super(Encoder, self).__init__()\n",
    "        # MLP with one hidden layer\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_topics)\n",
    "    \n",
    "    def forward(self, w):\n",
    "        # Apply softmax to output layer to get topic distribution theta\n",
    "        theta = F.softmax(self.fc2(F.relu(self.fc1(w))), dim=1)\n",
    "        return theta\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_topics, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Beta is the matrix of topic-word vectors (num_topics x vocab_size)\n",
    "        self.beta = nn.Parameter(torch.randn(num_topics, vocab_size))\n",
    "        # Bias term\n",
    "        self.bias = nn.Parameter(torch.randn(vocab_size))\n",
    "    \n",
    "    def forward(self, theta):\n",
    "        # Reconstruction of bag-of-words representation\n",
    "        logit = F.softmax(torch.matmul(theta, self.beta) + self.bias, dim=1)\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Wasserstein Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WAE(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_topics):\n",
    "        super(WAE, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, hidden_dim, num_topics)\n",
    "        self.decoder = Decoder(num_topics, vocab_size)\n",
    "\n",
    "    def forward(self, w):\n",
    "        theta = self.encoder(w)\n",
    "        recon = self.decoder(theta)\n",
    "        return recon, theta\n",
    "\n",
    "def loss_function(w, recon_w):\n",
    "    # Negative cross-entropy loss\n",
    "    return -torch.sum(w * torch.log1p(recon_w + 1e-10), dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the WAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute MMD loss\n",
    "def compute_mmd_loss(theta, prior, kernel):\n",
    "    n_samples = theta.size(0)\n",
    "    true_samples = prior.sample((n_samples,))\n",
    "    \n",
    "    xx = kernel(theta, theta).mean()\n",
    "    yy = kernel(true_samples, true_samples).mean()\n",
    "    zz = kernel(theta, true_samples).mean()\n",
    "    \n",
    "    return xx + yy - 2 * zz\n",
    "\n",
    "# Kernel function for MMD\n",
    "def mmd_kernel(theta, theta_prime, eps=1e-7):\n",
    "    # Ensure the inputs are on the correct device (theta is assumed to be on the correct device)\n",
    "    theta_prime = theta_prime.to(theta.device)\n",
    "    \n",
    "    # Compute the cosine similarity and ensure it is in the range [-1, 1] for numerical stability\n",
    "    cosine_similarity = torch.sum(torch.sqrt(theta + eps) * torch.sqrt(theta_prime + eps), dim=1)\n",
    "    cosine_similarity = torch.clamp(cosine_similarity, -1 + eps, 1 - eps)\n",
    "    \n",
    "    # Compute the kernel function\n",
    "    kernel_value = torch.exp(-torch.arccos(cosine_similarity) ** 2)\n",
    "    \n",
    "    return kernel_value\n",
    "\n",
    "\n",
    "# Function to add noise to the encoder output\n",
    "def add_noise_to_theta(theta, alpha, concentration):\n",
    "    noise = Dirichlet(concentration).sample((theta.size(0),)).to(device)\n",
    "    return (1 - alpha) * theta + alpha * noise\n",
    "\n",
    "# Modify the training loop to include MMD loss and noise\n",
    "def train(model, data_loader, optimizer, alpha, concentration, epochs=5):\n",
    "    model.train()\n",
    "    prior = Dirichlet(concentration)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data,) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            theta = model.encoder(data)\n",
    "            theta_noise = add_noise_to_theta(theta, alpha, concentration)\n",
    "            recon = model.decoder(theta_noise)\n",
    "            \n",
    "            recon_loss = loss_function(data, recon)\n",
    "            mmd_loss = compute_mmd_loss(theta, prior, mmd_kernel)\n",
    "            loss = recon_loss + mmd_loss  # Combine the reconstruction and MMD losses\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the TF-IDF matrix\n",
    "normalized_tfidf_matrix = normalize(tfidf_matrix)\n",
    "\n",
    "# Convert the normalized TF-IDF matrix to a dense PyTorch tensor\n",
    "tfidf_tensor = torch.from_numpy(normalized_tfidf_matrix.toarray()).float().to(device)  # Send to device\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = TensorDataset(tfidf_tensor)  # tfidf_tensor is already on the device\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the WAE model and send it to the device\n",
    "vocab_size = tfidf_tensor.shape[1]  # Number of features in the TF-IDF representation\n",
    "hidden_dim = 250\n",
    "num_topics = 5\n",
    "wae = WAE(vocab_size, hidden_dim, num_topics).to(device)\n",
    "\n",
    "# Choose an optimizer\n",
    "optimizer = torch.optim.Adam(wae.parameters(), betas=(0.99, 0.999), lr=0.002)\n",
    "\n",
    "# Example alpha and Dirichlet concentration parameter for noise, send to device\n",
    "alpha = 0.1\n",
    "concentration = torch.ones(num_topics, device=device)\n",
    "\n",
    "# Assuming that the train function correctly handles the device,\n",
    "# you can now call the train function with the new parameters:\n",
    "train(wae, data_loader, optimizer, alpha, concentration, epochs=10)\n",
    "\n",
    "# Save the model if needed\n",
    "# Note: When loading the model state, you'll need to map the location to the device\n",
    "torch.save(wae.state_dict(), 'wasserstein_ae.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wae.load_state_dict(torch.load('wasserstein_ae.pth'))\n",
    "\n",
    "# Assuming 'model' is already trained and 'tfidf_matrix' is available\n",
    "wae.eval()\n",
    "\n",
    "# Extract the latent representations\n",
    "latent_variables = []\n",
    "for i in range(tfidf_matrix.shape[0]):\n",
    "    data_point = torch.tensor(tfidf_matrix[i].toarray()).float().to(device)\n",
    "    _, z = wae(data_point)\n",
    "    latent_variables.append(z.detach().cpu().numpy())\n",
    "\n",
    "latent_variables = np.array(latent_variables).squeeze()\n",
    "\n",
    "# Use PCA for dimensionality reduction for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_latent = pca.fit_transform(latent_variables)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(reduced_latent[:, 0], reduced_latent[:, 1], alpha=0.5)\n",
    "plt.title('PCA of Latent Space')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=num_topics, random_state=0).fit(latent_variables)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(num_topics):\n",
    "    plt.scatter(reduced_latent[labels == i, 0], reduced_latent[labels == i, 1], label=f'Topic {i}', alpha=0.5)\n",
    "plt.title('Document Clusters in Latent Space')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the latent space to identify topics\n",
    "num_topics = 5\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=0).fit(latent_variables)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "words = vectorizer.get_feature_names_out()  # For sklearn versions 0.24 and later use get_feature_names_out()\n",
    "\n",
    "top_words_per_topic = []\n",
    "for i in range(num_topics):  # 'num_topics' is the number of clusters you decided in KMeans\n",
    "    # Calculate mean TF-IDF score for words in documents of each cluster\n",
    "    topic_mean = np.array(tfidf_matrix[labels == i].mean(axis=0)).flatten()\n",
    "    \n",
    "    # Get indices of top 10 words for this topic\n",
    "    top_indices = topic_mean.argsort()[-30:][::-1]  # Change 10 to another number if you want more or fewer words\n",
    "    top_features = [words[index] for index in top_indices]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Example output\n",
    "print(\"Top words per topic:\")\n",
    "for i, words in enumerate(top_words_per_topic):\n",
    "    print(f\"Topic {i}: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Assuming 'wae' is your trained model and 'vectorizer' is your TfidfVectorizer\n",
    "\n",
    "# First, get the beta matrix from the decoder (topic-word distribution)\n",
    "beta = wae.decoder.beta.cpu().detach().numpy()\n",
    "\n",
    "# Extract the top words for each topic\n",
    "top_n = 30  # Number of top terms to extract for each topic\n",
    "top_words = np.array(vectorizer.get_feature_names_out())\n",
    "top_words_topic = np.argsort(beta, axis=1)[:, -top_n:][::-1]\n",
    "topics = [top_words[topic_idxs] for topic_idxs in top_words_topic]\n",
    "\n",
    "# Convert the tokenized documents to a Gensim dictionary\n",
    "tokenized_docs = [doc.split() for doc in processed_texts]\n",
    "gensim_dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "# Convert tokenized documents into a document-term matrix for Gensim\n",
    "corpus = [gensim_dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Compute Coherence Score using Gensim's CoherenceModel\n",
    "coherence_model = CoherenceModel(topics=topics, texts=tokenized_docs, dictionary=gensim_dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f'Coherence Score: {coherence_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def topic_uniqueness_score(top_words_per_topic):\n",
    "    # Flatten the list of top_words_per_topic\n",
    "    all_words = [word for topic in top_words_per_topic for word in topic]\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    # Calculate the uniqueness score\n",
    "    uniqueness_scores = dict()\n",
    "    for i in range(len(top_words_per_topic)):\n",
    "        uniqueness_scores[i] = sum([1 / word_counts[word] for word in top_words_per_topic[i]])/len(top_words_per_topic[i])\n",
    "    return uniqueness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the latent space to identify topics\n",
    "print(topic_uniqueness_score(top_words_per_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(top_words, top_words_topic, topics)\n",
    "pprint.pp(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
